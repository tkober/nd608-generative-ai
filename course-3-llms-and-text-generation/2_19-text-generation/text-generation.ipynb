{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from helper import (\n",
    "    start_time,\n",
    "    time_since,\n",
    "    ShakespeareDataset,\n",
    "    TokenMapping,\n",
    "    build_model,\n",
    "    next_token,\n",
    "    # Character-based helpers\n",
    "    encode_text,\n",
    "    # Subword-based helpers\n",
    "    encode_text_from_tokenizer,\n",
    "    tokenize_text_from_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Deterministic training\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file: 50,085\n"
     ]
    }
   ],
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = './data/shakespeare_small.txt'\n",
    "\n",
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()\n",
    "\n",
    "print(f'Number of characters in text file: {len(raw_text):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Text Generation\n",
    "\n",
    "The first model you'll build for text generation will use character-based\n",
    "tokens.\n",
    "\n",
    "Each token will be a single character from the text and the model will learn\n",
    "to predict the next character (a token).\n",
    "\n",
    "To generate text, the model will take in a new string,\n",
    "character-by-character, and then generate a new likely character based on the\n",
    "past input. Then the model will take into account that new character and\n",
    "generate the following character and so on and so on until the model has\n",
    "produced a set number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "before we proceed any further, hear me speak.\n",
      "\n",
      "all:\n",
      "speak, speak.\n",
      "\n",
      "first citizen:\n",
      "you are all resolved rather to die than to famish?\n",
      "\n",
      "all:\n",
      "resolved. resolved.\n",
      "\n",
      "first citizen:\n",
      "first, you know caius marcius is chief enemy to the people.\n",
      "\n",
      "all:\n",
      "we know't, we know't.\n",
      "\n",
      "first citizen:\n",
      "let us kill him, and we'll have corn at our own price.\n",
      "is't a verdict?\n",
      "\n",
      "all:\n",
      "no more talking on't; let it be done: away, away!\n",
      "\n",
      "second citizen:\n",
      "one word, good citizens.\n",
      "\n",
      "first citizen:\n",
      "we are accounted poor\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    # TODO: Normalize incoming text; can be multiple actions\n",
    "    normalized_text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn').lower()\n",
    "    return normalized_text\n",
    "\n",
    "normalized_text = normalize_text(raw_text[:500])\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "def pretokenize_text(text: str) -> str | list[str]:\n",
    "    smaller_pieces = [ c for c in text ]\n",
    "    return smaller_pieces\n",
    "\n",
    "# TEST: Is your (normalized) text pretokenized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "pretokenized_text = pretokenize_text(normalized_text)\n",
    "print(pretokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps\n",
    "def tokenize_text(text: str) -> str | list[str]:\n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: str | list[str] = pretokenize_text(normalized_text)\n",
    "    # Characters are already tokens so pretokenized text is already tokenized\n",
    "    tokenized_text = pretokenized_text\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your tokenized text the way you expected?\n",
    "tokenized_text = tokenize_text(raw_text[:500])\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "We'll skip postprocessing since we don't have any special tokens we want to\n",
    "consider for our task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text()` from our helper module that can encode our text based on\n",
    "our tokenization process from our created `tokenize_text()` function.\n",
    "\n",
    "This will also provide us with `character_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, character_mapping = encode_text(raw_text, tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 50,086 characters\n",
      "Number of unique tokens: 37\n"
     ]
    }
   ],
   "source": [
    "n_tokens = character_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} characters')\n",
    "print(f'Number of unique tokens: {n_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_char()` function will use your tokenizer and NLP model to\n",
    "generate new text token-by-token (character-by-character in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_chars` parameter to tell the function how many tokens\n",
    "(characters in this case) to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_char(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = character_mapping,\n",
    "    num_chars: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Uses your character-based tokenizer\n",
    "    tokenized_text: list[str] = tokenize_text(input_str)\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_chars):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_char = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_char)\n",
    "    # Returns input string plus the full generated string (of generated tokens)\n",
    "    full_text = ''.join(tokenized_text + generated_tokens)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our character-based\n",
    "tokenizer) and attempt to predict the next token. Over time, the model should\n",
    "hopefully get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 2.53016718043306\n",
      "[00m 10.4s (0 0.0) 2.1864]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bedy doy ethinus,\n",
      "none onth hea fhind kinguc's thers, dher there weral, the mawerte.\n",
      "\n",
      "vier:\n",
      "wrant, apk\n",
      "Epoch 2/25, Loss: 2.181258386468735\n",
      "[00m 21.4s (1 4.0) 1.9870]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be weve hame stem the i crourcius,\n",
      "arcis,\n",
      "cir.\n",
      "ste fhakn wor\n",
      "huce theel whey al moire come\n",
      "at boment p\n",
      "Epoch 3/25, Loss: 2.078537760679714\n",
      "[00m 32.1s (2 8.0) 1.8850]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to berth brut,\n",
      "fot heverh serseng\n",
      "leend. and ofpor nais iais and of voitn maning of frenead,\n",
      "whingy whron\n",
      "Epoch 4/25, Loss: 2.0192134987431976\n",
      "[00m 43.0s (3 12.0) 1.8197]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beatlive theer the the ceplinenus:\n",
      "with, thy comnogh arous,\n",
      "uly sutar:\n",
      "hinsud,\n",
      "oud in my citeng\n",
      "the le\n",
      "Epoch 5/25, Loss: 1.9766919538235892\n",
      "[00m 51.7s (4 16.0) 1.7796]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet ontirs, athere'd poods\n",
      "its, the gor,\n",
      "now nor, woud othelq suedes ary bessill\n",
      "of whing, i dem grial\n",
      "Epoch 6/25, Loss: 1.9433630256987988\n",
      "[01m 2.9s (5 20.0) 1.7546]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to ben a, an ther; coug, thow nops loius to sof chaverove use,\n",
      "barwita'l:\n",
      "thests inder, the po fort;\n",
      "whe'\n",
      "Epoch 7/25, Loss: 1.9164595714392372\n",
      "[01m 13.9s (6 24.0) 1.7339]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be dowieshing, is, casgrerours avender non nop ones trot or thon'd hamcio hospee\n",
      "chather a ap ca part\n",
      "\n",
      "Epoch 8/25, Loss: 1.8943748218182939\n",
      "[01m 24.7s (7 28.000000000000004) 1.7176]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bead's canm asseld\n",
      "wuelfort\n",
      "mang pair to renter:\n",
      "all one i he come; the goo? or worther all\n",
      "ot, mis no\n",
      "Epoch 9/25, Loss: 1.8757073905140447\n",
      "[01m 39.9s (8 32.0) 1.7068]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beigh' do but,\n",
      "ciqkep, lasy more ereperears! plenily toges cait\n",
      "sutnolnee of the strand of he halglad \n",
      "Epoch 10/25, Loss: 1.8597314223694725\n",
      "[01m 51.9s (9 36.0) 1.6993]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be itses coucthe they the folddt fur bus or whatt and work\n",
      "at in the d'ded it gensby worthe,\n",
      "him: thy \n",
      "Epoch 11/25, Loss: 1.8458594296306086\n",
      "[02m 4.4s (10 40.0) 1.6941]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beires,\n",
      "got.\n",
      "\n",
      "firgonde'ss pleath with canchhey btem,\n",
      "and griest he reatt one ass trutu, sty is paigh i\n",
      "Epoch 12/25, Loss: 1.8337552704369298\n",
      "[02m 17.3s (11 44.0) 1.6896]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beirees for mis him but free in\n",
      "therhon is betitel--like the somin i jame, thtime; i wand\n",
      "divesenenius\n",
      "Epoch 13/25, Loss: 1.8230698924475965\n",
      "[02m 30.2s (12 48.0) 1.6847]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bets sto, liase ard\n",
      "now atts the us\n",
      "and man ageates you,\n",
      "that dod and to of ome:\n",
      "'the sitthonahed pove\n",
      "Epoch 14/25, Loss: 1.813555610065643\n",
      "[02m 43.5s (13 52.0) 1.6784]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bererdeces\n",
      "apinpets\n",
      "hereqippicer:\n",
      "say tfo the's cull to marcius,\n",
      "be ous ome my the him peterd enerseai\n",
      "Epoch 15/25, Loss: 1.8050946276789657\n",
      "[02m 55.7s (14 56.00000000000001) 1.6712]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be mave both that:\n",
      "but\n",
      "thour 'failea, the mamy?\n",
      "\n",
      "laphey, no drom?\n",
      "\n",
      "senots and comm not of that\n",
      "of mock\n",
      "Epoch 16/25, Loss: 1.7974912469760298\n",
      "[03m 8.4s (15 60.0) 1.6645]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be groonins'd\n",
      "soy\n",
      "thous soy wiser worle dived here\n",
      "have hatius:\n",
      "and peesley that proverot hames\n",
      "a se; \n",
      "Epoch 17/25, Loss: 1.7906103059506644\n",
      "[03m 22.5s (16 64.0) 1.6590]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet to plever:\n",
      "for of shale. are antryomes he hace thoper whand tume.\n",
      "\n",
      "mand he reves offer of thap it \n",
      "Epoch 18/25, Loss: 1.784351791589024\n",
      "[03m 35.4s (17 68.0) 1.6552]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be word thered tond and teon dey the riment\n",
      "flances come hundisgains, of if to nor the poppy flany the\n",
      "Epoch 19/25, Loss: 1.7786252104055387\n",
      "[03m 48.3s (18 72.0) 1.6521]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to betory?\n",
      "\n",
      "secondty for to their the faid to valees\n",
      "mand: would elderitigtin in that canmanging,\n",
      "genyigh\n",
      "Epoch 20/25, Loss: 1.7733724252865337\n",
      "[04m 2.6s (19 76.0) 1.6488]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be he'll do 'til to evercigited and af to.\n",
      "\n",
      "menate nods the, wellan: 'tis heartie and cane'n?\n",
      "\n",
      "marcius\n",
      "Epoch 21/25, Loss: 1.768535614166016\n",
      "[04m 17.0s (20 80.0) 1.6451]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to belon tonour of de in letch, gange,\n",
      "and tougts it let!\n",
      "\n",
      "vilple\n",
      "a, them'd here we and howe if theale, u\n",
      "Epoch 22/25, Loss: 1.7640523776459618\n",
      "[04m 32.5s (21 84.0) 1.6418]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to belves ond brathers\n",
      "is, he secouch of not.\n",
      "\n",
      "vongeed feally's: thouth herqomiunt,\n",
      "ict for btonly erever\n",
      "Epoch 23/25, Loss: 1.759862140801768\n",
      "[04m 45.6s (22 88.0) 1.6392]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be usonfer wellor the bleest, afee\n",
      "with, he hars thy to your they netguth were dive my he but is him; \n",
      "Epoch 24/25, Loss: 1.7559760076169388\n",
      "[04m 58.1s (23 92.0) 1.6372]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to besor: i test.\n",
      "\n",
      "meating of it heretited heeste mances to my the to thou up the for their may fo peir o\n",
      "Epoch 25/25, Loss: 1.7523777849758013\n",
      "[05m 9.7s (24 96.0) 1.6360]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bear and he gover\n",
      "to such deas, thoughy curgioce has poper defup,\n",
      "them mereeps.\n",
      "\n",
      "marcius geot's in bav\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text_by_char(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        num_chars=100,\n",
    "    )\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_chars`) to\n",
    "generate and observe how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to betold marcius\n",
      "come.\n",
      "\n",
      "unth thecius:\n",
      "what, done geak the mens afrumus,\n",
      "to done.\n",
      "\n",
      "fill onk\n",
      "say scolding \n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_char(\n",
    "    input_str='To be or not to be',\n",
    "    model=model,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    "    topk=None,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Text Generation\n",
    "\n",
    "The next model you'll build will use subword-tokenization instead of \n",
    "characters-based token to train a model and ultimately generate new text\n",
    "token-by-token.\n",
    "\n",
    "Although this could be done by creating your own tokenizer, you'll use\n",
    "Hugging Face to use a pretrained tokenizer to tokenize the data.\n",
    "\n",
    "After training the model with subword tokens, \n",
    "the model will take in a new string, token-by-token, and then generate a new\n",
    "token (subword).\n",
    "The model will continue producing new subword tokens based on the input text\n",
    "and already produced tokens until a set number of tokens have been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Tokenizer\n",
    "\n",
    "> NOTE:\n",
    "> \n",
    "> You can load another model outside of these choices but the model\n",
    "> will have to be downloaded and may or may not be effective.\n",
    ">\n",
    "> If you'd like to explore more, here's a link to you might want to start with\n",
    "> of different available pretrained models on Hugging Face:\n",
    "> https://huggingface.co/models?pipeline_tag=text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1269c6163140ef8d32e4846496bd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2c166f4ffb4db49cd9595f926d6b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ab236850c54a84948997303d92bcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6264eca6b7284dc6bc2a44726ebbbd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Choose a pretrained tokenizer to use:\n",
    "\n",
    "# Docs: https://huggingface.co/xlm-roberta-base\n",
    "# model_name = 'xlm-roberta-base'\n",
    "# DOCS: https://huggingface.co/bert-base-cased\n",
    "# model_name = 'bert-base-cased'\n",
    "# DOCS: https://huggingface.co/bert-base-uncased \n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text_from_tokenizer()` from our helper module that can encode\n",
    "our text based on our tokenization process from our tokenizer `my_tokenizer`.\n",
    "\n",
    "This will also provide us with `token_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, token_mapping = encode_text_from_tokenizer(\n",
    "    text=raw_text,\n",
    "    tokenizer=my_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 13,139 tokens\n"
     ]
    }
   ],
   "source": [
    "n_tokens = token_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of tokens\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_subword()` function will use your chosen tokenizer and the\n",
    "NLP model to generate new text token-by-token (subwords in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_tokens` parameter to tell the function how many\n",
    "(subword)tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_subword(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = token_mapping,\n",
    "    tokenizer = my_tokenizer,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Use your chosen subword-tokenizer\n",
    "    tokenized_text = tokenize_text_from_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        text=input_str,\n",
    "    )\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_tokens):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_token = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_token)\n",
    "    # List of all token IDs (input text and generated text)\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(\n",
    "        tokenized_text + generated_tokens\n",
    "    )\n",
    "    # Returns input string plus the full generated string from list of token IDs\n",
    "    full_text = tokenizer.decode(output_ids)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our subword tokenizer)\n",
    "and attempt to predict the next token. Over time, the model should hopefully\n",
    "get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 6.520003196669788\n",
      "[00m 8.5s (0 0.0) 5.5864]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beoran wives co words th shall je, so some know to promise nature because,, we indeed byli er too lord them is to : then the\n",
      "Epoch 2/25, Loss: 5.872647679724344\n",
      "[00m 17.0s (1 4.0) 4.9849]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be, you complaint have will you lose pre theirce, i of price pin s of : : himself and? : the said charmsjou to well we\n",
      "Epoch 3/25, Loss: 5.575401218926034\n",
      "[00m 24.8s (2 8.0) 4.5979]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be crown their so officers little day generosity. and, he place iron ' our name ofates. one lie hatred triumph moreria! what perform this because\n",
      "Epoch 4/25, Loss: 5.335869501858223\n",
      "[00m 33.2s (3 12.0) 4.3853]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to belanus :e poor purpose centuries were me vo abundant, sirus obey moths grave done poornia willh go, goodaneus : and s\n",
      "Epoch 5/25, Loss: 5.140703348415654\n",
      "[00m 41.9s (4 16.0) 4.2484]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be been indeed! com ins pass, and without, five in there we to s donens, and moon but thirstutus you mess the people the does\n",
      "Epoch 6/25, Loss: 4.976089289711743\n",
      "[00m 50.5s (5 20.0) 4.1349]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be hear sideue here must every she where our? who think.. too onlyiedrti? volum super with did ever, being how shall pound\n",
      "Epoch 7/25, Loss: 4.831616291185704\n",
      "[00m 58.7s (6 24.0) 4.0343]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be heard her country : he graveenius : so, visit affection such their owns now ' the wind better con : to him, ' won in\n",
      "Epoch 8/25, Loss: 4.7015644079301415\n",
      "[01m 7.0s (7 28.000000000000004) 3.9432]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be commit crack, honour? menenius : had words,! so mal been. cominius : the he goddess ' srio. meneni\n",
      "Epoch 9/25, Loss: 4.5824537195810455\n",
      "[01m 15.1s (8 32.0) 3.8576]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bes two ; he we have -ve inly ' grave my, of. youg applause for in ' d fight, not make receipt at him\n",
      "Epoch 10/25, Loss: 4.471694062977303\n",
      "[01m 23.4s (9 36.0) 3.7766]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be content die hiss, ' take, and a pays at to within bound andoe us danger coming of fare and enemies. they that see now\n",
      "Epoch 11/25, Loss: 4.367831744217291\n",
      "[01m 30.8s (10 40.0) 3.7033]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be daughter. menenius : even : here drumsing hisea ay betruct, ius : stallstal of rake breaths? - - '\n",
      "Epoch 12/25, Loss: 4.270051807310523\n",
      "[01m 39.3s (11 44.0) 3.6357]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be his undone at doubt a gulf : breathe a make entire a confessria : aufidius - beling still loving, but lovele if we rather\n",
      "Epoch 13/25, Loss: 4.177479912595051\n",
      "[01m 47.5s (12 48.0) 3.5716]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be! marcius, no shall thou consul ; for i thank you doors, god this wrath of hum me so atbus says for manybly and -\n",
      "Epoch 14/25, Loss: 4.089720223008133\n",
      "[01m 55.1s (13 52.0) 3.5095]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be commanded : though you speak ably, if you ' s leave two a quickly well of were, we buildingsies condition. sicinius : he\n",
      "Epoch 15/25, Loss: 4.006434309773329\n",
      "[02m 3.8s (14 56.00000000000001) 3.4496]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be wounded out not makes : them, - coming is ' tis know not welcome this shallly at your patience ; and the remove of hereditaryer the tongues\n",
      "Epoch 16/25, Loss: 3.927282433393525\n",
      "[02m 12.3s (15 60.0) 3.3930]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be moon matter, that ' d, that ' s appetite there on ' s lose the whole true, as in deeds. a shower the common to a\n",
      "Epoch 17/25, Loss: 3.85199599382354\n",
      "[02m 21.2s (16 64.0) 3.3398]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be angry the ore, that is a kindd. see, have neither about, who fall we ' s for ' no be hers had lack two\n",
      "Epoch 18/25, Loss: 3.780231265323918\n",
      "[02m 28.8s (17 68.0) 3.2888]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be some ce or with our man ' d? menenius! corioli to the nape, she stayns, though you ' llly shall\n",
      "Epoch 19/25, Loss: 3.711713194847107\n",
      "[02m 37.4s (18 72.0) 3.2401]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be lastspita shields any the reptag bound both ' s about for ' tis thet him sounds a placer. marcius, though ever dislike\n",
      "Epoch 20/25, Loss: 3.6462629492689924\n",
      "[02m 45.8s (19 76.0) 3.1935]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be goous should run to the leash, if ' s, he well of sound? menenius : he did for so! coriolanus\n",
      "Epoch 21/25, Loss: 3.583741919587298\n",
      "[02m 54.2s (20 80.0) 3.1480]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beseech on their years. you, thou infect this is the leg, here? volumnia :ourlyly ' s the coli long\n",
      "Epoch 22/25, Loss: 3.5240539353068283\n",
      "[03m 2.0s (21 84.0) 3.1032]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beriolanus and i will fall : no surelier below damask ' s a war tribunes for. if the discovery of my eyes, who\n",
      "Epoch 23/25, Loss: 3.467007655632205\n",
      "[03m 10.3s (22 88.0) 3.0598]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beten hell was an prescription of my? coriolanus : ever make r! all : that him, to very insurrection greyhound! aufidius\n",
      "Epoch 24/25, Loss: 3.412418732410524\n",
      "[03m 18.8s (23 92.0) 3.0174]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be angry we are otherwise : and co him does to proud not care me : : may, threatening, his spectacle in noon too, artss not valiant\n",
      "Epoch 25/25, Loss: 3.360066258616564\n",
      "[03m 27.0s (24 96.0) 2.9775]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be know the brain days of you ' ldsily, they are of every ear! where i have you will plucked. brutus : and them, and\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    output = generate_text_by_subword(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_tokens`)\n",
    "to generate and observe how it does.\n",
    "\n",
    "------------\n",
    "\n",
    "Consider how this model differs from the results from the text generation using\n",
    "the character-based tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be grafus as as well is say i was for it but a pp butterfly it you were hereditary not red him of such in him. cominius\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_subword(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.5,\n",
    "        topk=100,\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
