{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08f2e1c",
   "metadata": {
    "id": "a08f2e1c"
   },
   "source": [
    "# Building a Simple Web Scraper\n",
    "In this exercise, we will build a simple web scraper using:\n",
    "* the `requests` library for interacting with websites over HTTP\n",
    "* the `bs4` (aka BeautifulSoup4) library for interacting with HTML content\n",
    "* the `pathlib` library for nicely structuring our directory\n",
    "\n",
    "While many datasets are freely and openly available, specialized information is not always widely available.\n",
    "For this exercise, we will use [ToScrape books](http://books.toscrape.com/), a site that explicitly permits scraping.\n",
    "\n",
    "To collect our dataset, we will need to **generate a list of URLS to scrape** and then, for each URL:\n",
    "1. Get the page\n",
    "2. Extract the text components of the page\n",
    "3. Write the text to disk\n",
    "\n",
    "## Imports\n",
    "First, let's import the necessary libraries and objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4250c59",
   "metadata": {
    "id": "c4250c59"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c6da6",
   "metadata": {
    "id": "448c6da6"
   },
   "source": [
    "## Fetching a Page\n",
    "\n",
    "In order to write our loop, we will need to define exactly what elements we want to extract from a target page.\n",
    "However, we haven't even seen a single page yet!\n",
    "Let's define a function to do exactly that -- given a `url` parameter, fetch the page and return the body.\n",
    "\n",
    "If you're not familiar with the `requests` library, you can check the [quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/) documentation page.\n",
    "\n",
    "Although our sample site is designed to permit scraping, note that many websites will block requests from `requests`, so you may have to configure a `user-agent` string in your header.\n",
    "To verify that you have not been blocked, you can check the `.status_code` attribute of the `Response` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33073bb0",
   "metadata": {
    "id": "33073bb0"
   },
   "outputs": [],
   "source": [
    "def fetch_page(url: str):\n",
    "    headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # Todo: fetch the page using the GET requests\n",
    "    r =\n",
    "\n",
    "    # Todo check status code. Return the request body if code == 200, else print status code and return the body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e489cea",
   "metadata": {
    "id": "1e489cea"
   },
   "outputs": [],
   "source": [
    "# Test if the function fetch the page correctly\n",
    "test_url = \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
    "test_result = fetch_page(test_url)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e273716",
   "metadata": {
    "id": "2e273716"
   },
   "source": [
    "Nice! \n",
    "Now we see that there is a lot of information on this page, and much of it is not very useful for us -- especially for training a book description-writing language model!\n",
    "So let's go ahead and write a function to extract the relevant parts of the page using `BeautifulSoup`.\n",
    "\n",
    "## Parsing Web Pages\n",
    "Since we can fetch arbitrary webpages and we have a test result already stored -- let's write a function to extract only the text we want!\n",
    "Since our language model will be writing product descriptions, we want to extract the product description from each page. \n",
    "\n",
    "To do this, we'll need to extract the text inside the `<p>` tag following the `\"product_description\"` `<div>` tag.\n",
    "\n",
    "To navigate the tree, we'll need to use the `.find()` or `.find_all()` method of `BeautifulSoup`.\n",
    "\n",
    "Some tags have ids, so if we know the particular tag id, we can use the `id` keyword within `.find()`, like this: `soup.find('p', id='name')`\n",
    "In some cases, we want the *next* tag of a given type once we find the relevant part of the text, so we can use `.find_next()`.\n",
    "\n",
    "There are a few valid ways to do this, but if we inspect a few pages, we're very lucky that our product description always seems to be the `<p>` tag immediate after the `<div>` element with `id=\"product_description\"`. And we'll need the `.text` attribute of that `<p>` tag.\n",
    "\n",
    "If you need more details on how to use `BeautifulSoup`, you can find them in the library's [documentation](https://beautiful-soup-4.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c67a4",
   "metadata": {
    "id": "291c67a4"
   },
   "outputs": [],
   "source": [
    "def parse_page(html_doc: str):\n",
    "    # Todo: parse the html doc returned from fetch_page using BeautifulSoup\n",
    "    soup =\n",
    "\n",
    "    # Todo: find the text with <div> tag with production_description id\n",
    "    product_div =\n",
    "\n",
    "    # Todo: find the the <p> element that is immediate siblings of the product_div\n",
    "    selected_elements =\n",
    "\n",
    "    # Todo: return the attribute of the tag using .text\n",
    "    description =\n",
    "\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96591c75",
   "metadata": {
    "id": "96591c75"
   },
   "outputs": [],
   "source": [
    "# Check the product description\n",
    "test_text = parse_page(test_result)\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3730af",
   "metadata": {
    "id": "3d3730af"
   },
   "source": [
    "Great! Now we can see the product description in a string format.\n",
    "## Saving Files\n",
    "\n",
    "Once we've scraped and parsed the page, we want to save the raw data, in the form of text, to a file.\n",
    "For this, we want to specify a directory. We want to be able to specify where to save the raw data -- into a train or test directory.\n",
    "\n",
    "Since we have the URL of the file, we'll want to save each file according to the unique identifying string.\n",
    "For example, one of URLs are of the form `\"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"` so the file name we want is \"a-light-in-the-attic_1000.txt\", which is composed by the second last component of the URL \"a-light-in-the-attic_1000\" and a \".txt\" string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a374916",
   "metadata": {
    "id": "4a374916"
   },
   "outputs": [],
   "source": [
    "def save_text(text, url, train=True):\n",
    "    # Save the data to \"./data/train/\" if it's in the training set\n",
    "    if train:\n",
    "        file_path = Path(\"./data/train/\")\n",
    "        file_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # If data is not in the training set, save it to \"./data/test/\"\n",
    "    else:\n",
    "        file_path = Path(\"./data/test/\")\n",
    "        file_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Todo: split the URL by \"/\"\n",
    "    split_url = \n",
    "\n",
    "    # Todo: pull the name from the URL, and add a .txt extension to the end of the file\n",
    "    file_name =\n",
    "\n",
    "    # Write the file to disk\n",
    "    with open(file_path.joinpath(file_name), \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the save_file function\n",
    "# You shoude see file_name output as olio_984.txt\n",
    "# You should also find a exercise1/data/train folder with a file \"olio_984.txt\"\n",
    "save_text(test_text, test_url, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2770b6",
   "metadata": {
    "id": "8d2770b6"
   },
   "source": [
    "## Generating URLs\n",
    "\n",
    "We have the test URL for \"olio_984\", but since we want to collect all the books and pages, we'll need to generate URLs for each of them.\n",
    "\n",
    "Some sites have predictable page numbers and locations, but unfortunately, we'd need the name and index (*e.g.* a-light-in-the-attic_1000) to specify.\n",
    "\n",
    "Luckily, we can scrape these from the home page (and from subsequent pages if we wish, since those pages are sequential!) using the same `requests` and `BeautifulSoup` methods we've seen previously.\n",
    "\n",
    "In this case, we can re-use our `fetch_page` function and simply collect all the links on the page by using `BeautifulSoup`'s `.find_all()` method to get all of the `<a>` tags.\n",
    "\n",
    "For each tag, we'll want to access the `'href'` element to get the actual link text. Note that the URLs on this page are **relative**! That means they use `\"../../\"` instead of the full URL text.\n",
    "\n",
    "If we `.split()` the URL on `\"/\"`, we can find that the array for a book title has exactly **4 elements**. And URL for books starts with **`\"../../\"`** (but NOT `\"../../../\"`). For example: `\"../../set-me-free_988/index.html\"`. These two condisions let us return only URLs for books.\n",
    "\n",
    "Then, since our URLs are relative, we'll want to `.replace()` the relative reference with the appropriate prefix: `\"http://books.toscrape.com/catalogue/\"`. For example, the URL for \"set-me-free_988\" should be `\"http://books.toscrape.com/catalogue/set-me-free_988/index.html\"`.\n",
    "It's also possible that we have duplicates, and so we'll want to remove those where possible to minimize how much we scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178fd83",
   "metadata": {
    "id": "9178fd83"
   },
   "outputs": [],
   "source": [
    "def generate_url_list():\n",
    "    # Create a list to store our urls\n",
    "    url_list = list()\n",
    "    \n",
    "    # Specify the index page and fetch it\n",
    "    home = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
    "    home_page = fetch_page(home)\n",
    "    \n",
    "    # Todo: create a soup object for the home page using BeautifulSoup\n",
    "    soup =\n",
    "    \n",
    "    # Todo: find all the links on the page using the <a> tag and 'href' element\n",
    "    links =\n",
    "\n",
    "    for element in links:\n",
    "        # Todo: in the if statement, find the condition where element['href'] has 4 elements, \n",
    "        # contains \"../../\", but not \"../../../\"\n",
    "        if ...\n",
    "            # Extract the url with the relative (..) references\n",
    "            relative_url = element['href']\n",
    "            \n",
    "            # Todo: replace the relative references \"../../\" \n",
    "            # with the base URL \"http://books.toscrape.com/catalogue/\"\n",
    "            full_url = \n",
    "            \n",
    "            # Append the URL to the url_list\n",
    "            url_list.append(full_url)\n",
    "    # Deduplicate links in the list\n",
    "    url_list = list(set(url_list))\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdfc3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the urls are valid\n",
    "url_list = generate_url_list()\n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48f77d",
   "metadata": {
    "id": "ea48f77d"
   },
   "source": [
    "## Bringing It All Together\n",
    "Once we have our list of (probably!) valid URLs, we'll want to bring it all together.\n",
    "\n",
    "First, generate your url list. You'll want to make sure that your URL is valid since the provided URLs were relative. Then, iterate over it to fetch the product description for each book and save the text into text files.\n",
    "\n",
    "Before writing the code, let's do some simple tests to make sure the function you wrote are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad46fbf",
   "metadata": {
    "id": "3ad46fbf"
   },
   "outputs": [],
   "source": [
    "# Test if the fetch_page and parse_page functions run correctly.\n",
    "# Run the cell a few times to test if the descrption is extracted successfully on a random url from the url_list\n",
    "import random\n",
    "url = random.choice(url_list)\n",
    "\n",
    "page_text = fetch_page(url)\n",
    "product_description = parse_page(page_text)\n",
    "print(url + \"\\n\")\n",
    "print(product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6176c8a",
   "metadata": {
    "id": "e6176c8a"
   },
   "outputs": [],
   "source": [
    "# Bring it all together to production description texts from mupliple urls and save them to the disk\n",
    "for url in url_list:\n",
    "    page_text = fetch_page(url)\n",
    "    product_description = parse_page(page_text)\n",
    "    save_text(product_description, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b7989e",
   "metadata": {
    "id": "d7b7989e"
   },
   "source": [
    "Now if you go back to the `exercise1/data/train` directory, you will see the descriptions are stored in many text files, with the correspoinding file name."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
